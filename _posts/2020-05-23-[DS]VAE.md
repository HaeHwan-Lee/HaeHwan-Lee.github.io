---
title: <i class="far fa-chart-bar"> [Paper Review] Variational Autoencoder </i>
date: 2020-05-23 20:04:00 +0800
categories: [Data Science, Machine Learning]
tags: [Machine Learning, VAE]
toc: true
comments: true
sitemap:
  changefreq: daily
  priority: 1.0
---

<b>VAE</b>(Variational Autoencoder)는 2013년 킹마(Kingma)가 처음 소개한 generative model입니다. Generative model은 주어진 데이터 자체의 확률분포를 구함으로써 새롭게 데이터를 생성하는 모델입니다. Generative model에 대한 내용은 [이 곳 포스트](https://haehwan.github.io/posts/DS-GenerativeModel/)에서 확인할 수 있습니다. 이번 글에서는 arXiv에 제출된 <b>[Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf)</b> 논문을 리뷰합니다. VAE가 어떤 기술이고, 어떤 원리에 의해 작동하는지 등에 대해 다루겠습니다.


# Motivation: Generative model의 성능을 어떻게 높일 수 있을까?   

 > How can we perform <b>efficient approximate inference</b> and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions? - ***kingma***

Kingma의 VAE 논문은 Approximate Inference를 효율적으로 수행하기 위한 고민에서부터 출발합니다. 결과적으로 그는 variational inference란 방법으로 이를 해결했고, 자신의 모델에 "Variational" Autoencoder라는 이름까지 붙여줍니다. 그만큼 VAE를 이해하기 위해서는 Approximate inference가 무엇이고, Variational Inference는 어떻게 작동하는지 등을 이해하는 것이 핵심입니다.

사실 Approximate Inference를 가장 많이 연구하는 학문 중 하나는 <b>베이즈 통계학</b>입니다. 실제로, variational inference 또한 베이즈 통계학에 뿌리를 두고 있습니다. 따라서 베이즈 통계학에 대한 간단한 소개부터 시작하려고 합니다.

## Bayesian Statistics

통계학에서는 모수를 추정하는 방식에 따라 두 가지 학파가 있습니다. 빈도주의 통계학에서는 모수를 상수로 취급합니다. 반면, 베이지안은 이를 확률변수로 다룹니다. 사소해 보이지만 어떠한 관점을 취하냐에 따라서 상당히 큰 차이가 만들어집니다. 예를 들어, 누군가 주사위에서 숫자 1이 나올 확률을 물어본다고 가정하겠습니다. 우리가 가지고 있는 데이터는 동일한 주사위를 6번 던져서 1이 2번 나왔다는 사실만을 알고 있습니다.

만약 이 확률을 상수로 취급한다면, 우리가 대답할 수 있는 모수에 대한 추정은, MLE 추정방법이 가장 대표적입니다. 이는 모수가 어떤 상수값을 가지고 있을 때, 지금 가지고 있는 데이터가 나타났을 확률이 가장 큰지 계산하는 방법입니다. 결국 요약하면, $$P(X \mid \theta)$$를 최대화해주는 $$\theta$$를 찾아내는 것이 핵심이 됩니다. 따라서 이 문제에서 빈도주의자들은 1/3이라고 대답할 것입니다.

반면에 베이지안은 모수를 확률변수라고 생각하기 때문에, 똑같은 질문에 대한 답으로 "분포"를 제시합니다. 쉽게 말해서 이들은 1/3과 같은 상수로 답변을 하는 것이 아니라, 0과 1 사이의 범위에서 정의된 분포로 대답합니다. 예를 들어서, 주사위의 1이 나올 가능성을 1/3이 50%, 2/3이 25%, 1/6이 25%라고 대답하는 식입니다. 

그리고 이 대답은 같은 베이지안이라고 할지라도 연구자에 따라 다양하게 나타납니다. 예를 들어, 누군가는 1/3에서 굉장히 높은 값을 가지는 분포로, 다른 누구는 데이터와 상관없이 0과 1 사이에서 평평한 uniform 분포로 답변할 수 있습니다. 단 이 때에, 내가 제시한 답안이 오직 <b>Bayes' Rule(Bayes' theorem)</b>을 만족만 하고 있으면, 어떠한 대답도 정답이 될 수 있습니다. 요약하면 이들은 <b>$$P(\theta \mid X)$$</b>를 찾아낸다고 말할 수 있습니다. 그리고 이를 가리켜, posterior 분포라고 이야기합니다.

다양한 대답이 나오는 이유는, 베이지안의 큰 장점 중 하나인 직관성과 관련이 깊습니다. 이를 위해서는 posterior 분포를 prior distribution, likelihood 그리고 normalizing constant에 대한 식으로 재구성할 필요가 있습니다. 

|![](https://wikimedia.org/api/rest_v1/media/math/render/svg/87c061fe1c7430a5201eef3fa50f9d00eac78810)|
|:--:|
|*Bayes' Rule*|


위의 식을 통해서 우리는 베이지안들이 사실은, 빈도주의자들이 구했던 조건부 기대(likelihood)에 더해서, prior 분포와 정규화 상수를 계산한다는 것을 알 수 있습니다. 앞에서 베이지안의 정답이 여러개가 가능한 이유가 사실 바로 이 prior 분포 때문입니다. prior 분포란, 주어진 데이터와 무관하게 연구자가 모수에 대해서 가지는 확률분포라고 이야기할 수 있습니다. 예를 들어, 주어진 정보를 많이 신뢰하는 사람이 있을 수도 있지만, 반대로 이전에 본인이 알고 있던 지식에 더 큰 비중을 두는 사람이 있을 수도 있습니다. 전자의 경우가 주사위 예시에서 1/3에서 뾰족한 분포를 정답으로 생각하게 되고, 만약 후자의 사람이 주사위가 매우 공정하다는 정보나 믿음이 있다면 0과 1의 모든 값에서 평평한 확률 분포를 도출해낼 것입니다. 이처럼 베이지안은 연구자의 믿음이나 지식에 따라서 다양한 분포가 나온다고 할 수 있습니다. 결국 어느 관점이 더 합리적이고 유용한가는 연구자의 선택에 따라 다르겠지만, 베이지안은 빈도주의자에 비해 직관적이고, 설명력을 가지며, 정보의 업데이트가 편리하다는 큰 장점을 가지고 있음을 알 수 있습니다.

## Approximate Inference

이러한 뛰어난 장점에도 불구하고, 베이즈 정리에서 알 수 있듯이, 베이즈 통계학에는 빈도주의자보다 필요로 하는 계산식이 더 많습니다. 특히 그 중에서 문제가 되는 것은, normalizing constant입니다. conjugate prior와 같이 아주 적은 경우를 제외하고 거의 모든 문제에서 이 <b>normalizing constant를 구하는 것이 불가능</b>하기 때문입니다. 그리고 이 때문에 특히 posterior 분포를 바탕으로 조건부 평균이라든가 분산 혹은 확률과 같은 특정 '값'들을 의논하는 것이 더욱 큰 난관에 봉착하게 됩니다. 조금 더 자세히 살펴보겠습니다.

우선 베이지안은 posterior 분포에 관심을 갖기 때문에, 평균과 분산과 같이 특정 '값'에 대해서 말할 때에도, 주어진 데이터를 조건으로 하는 값들에 대해서 관심을 갖습니다. 그리고 이들은 아래와 같은 과정으로 요약해서 구할 수가 있습니다. 

$$\mathbb{E}[h(X\mid Y)] = \int_{\mathcal{X}} X \mathbb{P}(X|Y) dX$$

여기서 만약 h 함수가 identity 함수라면, 이는 평균값을 의미합니다. 또는 $$x-\mu$$라면 분산이, 혹은 $$I(X>a)$$와 같은 indicator 함수라면 확률로 이해할 수 있습니다.

물론 이 적분은 가능한 모든 X의 공간인 '$$\mathcal{X}$$'에서 수행되어야합니다. 하지만 우리가 알듯이 이 적분이 어렵기 때문에, 다른 방법들을 사용해야 합니다. 이러한 방법들을 가리켜서 <b>approximate inference</b>라고 부릅니다. 즉, 근사적으로 원하는 형태의 기댓값들을 구해가는 일련의 과정들이라고 부를 수 있습니다.

### Markov Chain Monte Carlo

가장 대표적인 방법이 그 유명한 Markov Chain Monte Carlo(MCMC)입니다. 오늘은 VAE에 대한 내용을 다루기 때문에 자세히 다루지는 않겠습니다. 핵심적인 내용을 살펴보면, 다음과 같습니다. 먼저, 우리는 Law of Large numbers(대수의 법칙)을 활용해서 위의 식을 아래와 같이 대체할 수 있습니다. 이러한 방법을 가리켜서 Monte Carlo라고 부릅니다.

$$\int_{\mathcal{X}} X \mathbb{P}(X|Y) dX \sim \frac{1}{N} \sum_{i=1}^N X_i$$

단, 문제는 Monte Carlo를 쓰기 위해서는 내가 가지고있는 샘플, $$X_i$$가 해당 <b>posterior분포로부터 구해진 iid 샘플</b>이어야한다는 가정이 필요하다는 점입니다. 당연히 문제는 그 점이 어렵다는 사실입니다. 우선 우리가 현실적으로 샘플링할 수 있는 분포가 몇 개 없습니다. 정확히 말하면, R이나 파이썬 등에서 구현된 '유명한' 분포들이 샘플링 가능한 전부라고 말할 수 있습니다. 따라서 현실적으로 마주하게되는 대다수 posterior 분포들로부터 샘플을, 그것도 독립적으로 뽑아낸다는 것은 매우 힘든 일이 됩니다. MCMC 알고리즘은 몇 가지 가정을 바탕으로 정확한 posterior 분포가 아니라, 이와 비례한 어떠한 분포(prior와 likelihood의 곱)만 있다고 하더라도, 일정 기간 이후에는 어떠한 분포가 결국에는 정확한 posterior 분포로 수렴한다는 성질을 이용한 알고리즘입니다. 이를 통해서, 반복적인 시뮬레이션으로 결국은 샘플링을 해낼 수 있다는 것이 MCMC의 핵심입니다. 워낙 중요하기도 하고, 활용분야도 많다보니, 다음 기회에 더 자세히 다루도록 하겠습니다.

### Variational Inference

그리고 또 다른 대표적인 방법이 바로 오늘 다루게 될, Variational Inference입니다. 앞에서 잠깐 언급했던, MCMC가 독립적인 샘플링을 할 때에, 마치 순열값들을 쓰듯이, 확률 과정(stochastic process)을 통해서 posterior 분포로 수렴했다면, VI는 deterministic 방법이라고 말할 수 있습니다. 이를 이해하기 위해서는 MCMC와 비교를 통해서 감을 빨리 잡을 수 있습니다.

앞에서 MCMC는 결국에는 posterior 분포로 완벽히 수렴해갈 수 있음이 증명되어있습니다. 물론 몇 가지 가정을 만족해야하지만, starting point를 여러개 잡기만 한다면, 이러한 가정은 쉽게 대체될 수 있습니다. 정확한 값을 추론할 수 있지만 당연히 단점은 많은 계산량을 수반한다는 단점이 있습니다. 자연히 비교적 작은 크기의 문제를 해결하는데 초점이 맞춰지게 됩니다.

반면에 VI는 처음부터 posterior 분포를 근사적으로 구하고자 합니다. 애초에 정확한 값을 추론하는 것이 아니라, 내가 잘 다룰 수 있는 어떠한 분포로 대체해서 표현하는 것이기 때문에 계산량과 속도측면에서 훨씬 우월합니다. 자연히 단순한 데이터가 아니라 차원이 많거나, 크기가 큰 여러 현실 문제에서 잘 응용이 됩니다.

또 한 가지 장점은 VI로 문제를 대체하게 되면, 문제 유형이 바뀐다는 점입니다. 앞에서 말했듯, VI는 내가 잘 아는 여러 분포들 중에 실제 posterior 분포와 가장 근사한 분포를 찾아서 이를 대신해서 사용합니다. 대체해서 쓰는 분포가 실제 분포와 유사해야하기 때문에 이 때에는 divergence를 활용해서 분포 간의 차이를 나타내게 됩니다. divergence란, distance와 유사한 개념으로 생각하면 됩니다. 일종의 차이를 표현하는 수단입니다. 중요한 점은 이렇게 divergence로 분포와 분포 사이의 차이를 나타내었으면, 이를 최소화하는 일이 남았다는 것입니다. 그리고 이렇게 최소화해서 나온 분포로 posterior를 대체해서 씁니다. 결국, 위 문제는 에러나 비용을 최소화하는 것과 유사한, 일종의 최적화 알고리즘 문제로 바뀌게 된다는 점입니다. 이 때문에 VI가 속도 측명에서 훨씬 뛰어난 강점을 지닌다고 할 수 있습니다.



<br>  

***
***
# 각주 및 추천자료

## 추천자료 / 참고자료
1. [Applications of the Gauss-Newton Method](https://ccrma.stanford.edu/~wherman/tulane/gauss_newton.pdf)  

2. [convex optimization: Machine Learning](http://www.stat.cmu.edu/~ryantibs/convexopt/)

3. [한국어자료](https://darkpgmr.tistory.com/58)
## 각주


<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
